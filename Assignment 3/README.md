# 自动分词实验
## 一、实验目的
通过基于词典的最大匹配分词方法，实现对中文生语料的自动分词任务。实验基于Python编程语言，利用已标注的分词语料构建词典资源，并采用正向最大匹配算法对未分词的原始文本进行处理，最终输出符合规范的分词结果文件，为后续自然语言处理任务提供基础数据支持。

## 二、实验过程
词典构建阶段，程序读取预先以斜杠分隔的分词文件（dic.txt），通过逐行解析提取所有独立词语，利用集合数据结构实现词语去重，同时动态记录词典中最长词语的长度。这一设计确保了词典的唯一性，还优化了后续分词算法的匹配效率。经统计，实验构建的词典共包含若干词语（具体数量取决于 侯兆晗.txt 内容），最大词长为N个字符，这一关键参数将直接影响分词算法的窗口滑动策略。  
生语料处理阶段，程序采用正向最大匹配算法对原始文本（dic.txt）进行分词。该算法从待分词文本的起始位置开始，以词典最大词长为滑动窗口，依次尝试匹配最长可能词语。若匹配成功则切分该词并移动窗口，否则缩减窗口长度继续匹配，最终对未登录词执行单字切分策略。这种处理方式在保证基础分词准确率的同时，有效平衡了算法效率与未登录词处理问题。程序逐行处理输入文本，将分词结果以斜杠连接后写入输出文件（output.txt），完整保留原文的行结构特征。

## 三、实验结果
实验结果保存到output.txt文件中。  
该分词脚本能够较好地完成基础分词任务。输出文件中的词语切分边界与词典定义保持高度一致，长词语优先切分的策略避免了错误切分问题，而未登录词的单字切分方案则确保了文本信息的完整性。系统性能与词典质量密切相关：词典收录的词语越完备，未登录词比例越低，分词准确率则越高。同时最大词长的动态获取机制使算法能自适应不同领域的语料特征，避免了固定窗口长度导致的匹配效率损失。

## 四、实验结论
本次实验验证了基于规则的分词方法在可控场景下的有效性，但也暴露出对未登录词处理较为机械的局限性。后续改进可考虑引入统计语言模型对未登录词进行概率预测，或结合双向匹配算法提升切分准确性。此外，当前系统采用逐行处理的串行方式，在处理大规模文本时可能存在效率瓶颈，未来可通过并行化处理或引入更高效的数据结构进行优化。总体而言，该实验为理解基础分词原理提供了实践基础，其模块化设计也便于后续功能扩展与算法改进。
